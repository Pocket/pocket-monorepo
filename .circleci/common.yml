version: 2.1

orbs:
  aws-cli: circleci/aws-cli@4.1.3
  aws-ecr: circleci/aws-ecr@7.3.0
  aws-code-deploy: circleci/aws-code-deploy@3.0.0
  aws-ecs: circleci/aws-ecs@4.0.0

# This is an enum that is used within all our jobs and our exit early job.
# As a new "service/deployment" is added you should add to the enum.
# Then each job you pass a "for" to, so that we can determine if this job is for this "commit"
repo_for_enum: &repo_for_enum
  for:
    description: which repo this job is relevant for
    type: enum
    enum: 
      - image_api
      - annotations_api
      - shared_snowplow_consumer
      - parser_graphql_wrapper
      - transactional_emails
      - fxa_webhook_proxy
      - user_api
      - list_api
      - client_api
      - feature_flags
      - sendgrid_data
      - account_data_deleter
      - account_delete_monitor
      - shareable_lists_api
      - pocket_event_bridge
      - user_list_search
      - braze
      - v3_proxy_api
      - push_server

resource_class_enmum: &resource_class_enmum
  resource-class:
    description: The self hosted runnner to run on
    type: enum
    enum: 
      - pocket/default-dev
      - pocket/default-prod


parameters:
  image_api:
    type: boolean
    default: false
  annotations_api:
    type: boolean
    default: false
  shared_snowplow_consumer:
    type: boolean
    default: false
  parser_graphql_wrapper:
    type: boolean
    default: false
  transactional_emails:
    type: boolean
    default: false
  fxa_webhook_proxy:
    type: boolean
    default: false
  user_api:
    type: boolean
    default: false
  client_api:
    type: boolean
    default: false
  list_api:
    type: boolean
    default: false
  feature_flags:
    type: boolean
    default: false
  sendgrid_data:
    type: boolean
    default: false
  account_data_deleter:
    type: boolean
    default: false
  account_delete_monitor:
   type: boolean
   default: false
  shareable_lists_api:
    type: boolean
    default: false
  pocket_event_bridge:
    type: boolean
    default: false
  user_list_search:
    type: boolean
    default: false
  braze:
    type: boolean
    default: false
  v3_proxy_api:
    type: boolean
    default: false
  push_server:
    type: boolean
    default: false

commands:
  # Refrenced from https://github.com/kelvintaywl-cci/dynamic-config-showcase/blob/main/.circleci/next.yml
  exit-early-if-irrelevant:
    parameters:
      <<: *repo_for_enum
    steps:
      - run:
          name: stop early unless relevant
          command: |
            # looks up the relevant pipeline parameter via the env var
            export RELEVANT=$(eval echo "\$<< parameters.for >>")

            # NOTE: env var values are strings (not boolean)
            if [ "${RELEVANT}" = "1" ]; then
              echo "continuing, since job is for << parameters.for >>"
            else
              echo "stopping early!"
              circleci-agent step halt
            fi
          environment:
            image_api: << pipeline.parameters.image_api >>
            annotations_api: << pipeline.parameters.annotations_api >>
            shared_snowplow_consumer: << pipeline.parameters.shared_snowplow_consumer >>
            parser_graphql_wrapper: << pipeline.parameters.parser_graphql_wrapper >>
            transactional_emails: << pipeline.parameters.transactional_emails >>
            fxa_webhook_proxy: << pipeline.parameters.fxa_webhook_proxy >>
            user_api: << pipeline.parameters.user_api >>
            list_api: << pipeline.parameters.list_api >>
            client_api: << pipeline.parameters.client_api >>
            feature_flags: << pipeline.parameters.feature_flags >>
            sendgrid_data: << pipeline.parameters.sendgrid_data >>
            account_data_deleter: << pipeline.parameters.account_data_deleter >>
            account_delete_monitor: << pipeline.parameters.account_delete_monitor >>
            shareable_lists_api: << pipeline.parameters.shareable_lists_api >>
            pocket_event_bridge: << pipeline.parameters.pocket_event_bridge >>
            user_list_search: << pipeline.parameters.user_list_search >>
            braze: << pipeline.parameters.braze >>
            v3_proxy_api: << pipeline.parameters.v3_proxy_api >>
            push_server: << pipeline.parameters.push_server >>

  install_pnpm:  
    steps:
      - restore_cache:
          name: Restore pnpm Package Cache
          keys:
            - pnpm-packages-{{ checksum "pnpm-lock.yaml" }}
      - run:
          name: Install pnpm package manager
          command: |
            corepack prepare pnpm@latest-8 --activate
            pnpm config set store-dir .pnpm-store
      - run:  
          name: Install Dependencies
          command: |
            pnpm install
      - save_cache:
          name: Save pnpm Package Cache
          key: pnpm-packages-{{ checksum "pnpm-lock.yaml" }}
          paths:
            - .pnpm-store
  install_infrastructure_pnpm:
    steps:
      - restore_cache:
          name: Restore pnpm Package Cache
          keys:
            - pnpm-packages-{{ checksum "pnpm-lock.yaml" }}
      - run: 
          name: Install and setup node
          command: |
            nvm install
            nvm use
            npm install -g pnpm
            pnpm config set store-dir .pnpm-store
            pnpm install    
      - save_cache:
          name: Save pnpm Package Cache
          key: pnpm-packages-{{ checksum "pnpm-lock.yaml" }}
          paths:
            - .pnpm-store 
  install_codebuild_secrets:
    steps:
      - run: 
          name: Setup our secrets from AWS Secret Manager
          command: |
            echo 'export SECRET_VALUE="$(aws secretsmanager get-secret-value --secret-id CodeBuild/Default --query SecretString --output text)"' >> "$BASH_ENV"
            echo 'export TERRAFORM_TOKEN="$(echo $SECRET_VALUE | jq -r '.terraform_token')"' >> "$BASH_ENV"
            echo 'export PAGERDUTY_TOKEN="$(echo $SECRET_VALUE | jq -r '.mozilla_pagerduty_token')"' >> "$BASH_ENV"
      - run:
          name: Save off terraform token
          command: |
            echo Setting Up Terraform Token
            rc="credentials \"app.terraform.io\" { "
            rc="${rc} token=\"$TERRAFORM_TOKEN\" "
            rc="${rc}}"
            echo "$rc" > ~/.terraformrc

  setup_github_bot:
    steps:
      - run:
          name: Get Github Bot Token
          command: |
            app_id=$GITHUB_APP_ID
            pem="$(echo "$GITHUB_APP_PRIVATE_KEY" | base64 -d)"
            installation_id=$GITHUB_INSTALLATION_APP_ID

            now=$(date +%s)
            iat=$((${now} - 60)) # Issues 60 seconds in the past
            exp=$((${now} + 600)) # Expires 15 minutes in the future

            b64enc() { openssl base64 | tr -d '=' | tr '/+' '_-' | tr -d '\n'; }

            header_json='{
                "typ":"JWT",
                "alg":"RS256"
            }'
            # Header encode
            header=$( echo -n "${header_json}" | b64enc )

            payload_json='{
                "iat":'"${iat}"',
                "exp":'"${exp}"',
                "iss":'"${app_id}"'
            }'
            # Payload encode
            payload=$( echo -n "${payload_json}" | b64enc )

            # Signature
            header_payload="${header}"."${payload}"
            signature=$( 
                openssl dgst -sha256 -sign <(echo -n "${pem}") \
                <(echo -n "${header_payload}") | b64enc 
            )

            # Create JWT
            JWT="${header_payload}"."${signature}"

            # Make a POST request to GitHub API to get the installation token
            response=$(curl -s -X POST \
              -H "Accept: application/vnd.github.v3+json" \
              -H "Authorization: Bearer $JWT" \
              -d "{}" \
              "https://api.github.com/app/installations/$installation_id/access_tokens")

            # Extract the token from the response
            token=$(echo "$response" | jq -r '.token')
            echo "export GITHUB_TOKEN=$token" >> $BASH_ENV
            echo "export GH_TOKEN=$token" >> $BASH_ENV
            echo "export GITHUB_ACCESS_TOKEN=$token" >> $BASH_ENV


jobs:

  infrastructure:
    description: Build and optionally deploy the infratructure
    parameters:
      scope:
        description: The pnpm scope to build for
        type: string
        default: ''
      stack-output-path:
        description: The pnpm output path
        type: string
      apply:
        description: If you should apply
        type: boolean
        default: false
      dev:
        description: Whether or not its a dev build
        type: boolean
        default: false
      uses_raw_hcl:
        description: Signals that we do not need to compile the app code
        type: boolean
        default: false
      <<: [*repo_for_enum, *resource_class_enmum]
    # Our self hosted runners dont support docker images, cause its not deployed in kubernetes, so we have some special steps
    machine: true
    shell: /bin/bash --login -eo pipefail
    resource_class: << parameters.resource-class >>
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - checkout
      - unless:
          # When it is not raw hcl, the stack output path is 3 folders away from our .terraform-version
          condition: <<parameters.uses_raw_hcl>>
          steps:
            - restore_cache:
                name: Restore Terraform
                keys:
                  - terraform-version-{{ checksum "<< parameters.stack-output-path >>/../../../.terraform-version" }}
      - when:
          # When it is hcl, the stack output path is the same as our .terraform-version
          condition: <<parameters.uses_raw_hcl>>
          steps:
            - restore_cache:
                name: Restore Terraform
                keys:
                  - terraform-version-{{ checksum "<< parameters.stack-output-path >>/.terraform-version" }}
      - run:
          name: Install tfcmt
          command: |
            if ! command -v tfcmt &> /dev/null; then
                echo "tfcmt does not exist, installing"
                curl -L https://github.com/suzuki-shunsuke/tfcmt/releases/download/v4.9.0/tfcmt_linux_amd64.tar.gz | tar xvzf - tfcmt
                mv tfcmt /home/circleci/tfcmt
                chmod a+x /home/circleci/tfcmt
            fi
      - install_codebuild_secrets
      - setup_github_bot
      - unless:
          # Only compile if we do not use raw hcl
          condition: <<parameters.uses_raw_hcl>>
          steps:
            - install_infrastructure_pnpm
            - when:
                condition: <<parameters.dev>>
                steps:
                  - run:
                      name: Build Dev Infra
                      command: |
                        nvm use
                        export NODE_ENV=development
                        pnpm run synth --filter=<< parameters.scope >>...
            - unless:
                condition: <<parameters.dev>>
                steps:
                  - run:
                      name: Build Prod Infra
                      command: |
                        nvm use
                        export NODE_ENV=production
                        pnpm run synth --filter=<< parameters.scope >>...
      - when:
          condition: <<parameters.uses_raw_hcl>>
          steps:
          - when:
              condition: <<parameters.dev>>
              steps:
                - run:
                    name: Copy Dev tfvars
                    command: |
                      cd << parameters.stack-output-path >>
                      cp dev_backend.tfvars backend.tf
          - unless:
              condition: <<parameters.dev>>
              steps:
                - run:
                    name: Copy Prod tfvars
                    command: |
                      cd << parameters.stack-output-path >>
                      cp prod_backend.tfvars backend.tf         
      - run:
          name: Setup terraform
          command: |
            cd << parameters.stack-output-path >>
            tfenv use
            terraform init
      - when:
          condition: <<parameters.apply>>
          steps:
            - run:
                name: Terraform apply
                # Re-add this when tfcmt supports ignoring no change applies
                # https://github.com/suzuki-shunsuke/tfcmt/issues/1184
                #  /home/circleci/tfcmt --var target:<< parameters.scope >><<#parameters.dev>>-dev<</parameters.dev>> apply -- terraform apply -auto-approve -lock-timeout=10m
                command: |
                  cd << parameters.stack-output-path >>
                  terraform apply -auto-approve -lock-timeout=10m
                  mkdir -p /tmp/workspace
                  echo "$(terraform output -json)" > /tmp/workspace/tf_output.json
            # Persist TF_OUTPUT using workspace
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - tf_output.json
      - unless:
          condition: <<parameters.apply>>
          steps:
            - run:
                name: Terraform plan
                command: |
                  cd << parameters.stack-output-path >>
                  tfcmt --var target:<< parameters.scope >><<#parameters.dev>>-dev<</parameters.dev>> plan --skip-no-changes --patch -- terraform plan -lock-timeout=10m
      - unless:
          # When it is not raw hcl, the stack output path is 3 folders away from our .terraform-version
          condition: <<parameters.uses_raw_hcl>>
          steps:
            - save_cache:
                key: terraform-version-{{ checksum "<< parameters.stack-output-path >>/../../../.terraform-version" }}
                paths:
                  - /home/circleci/.tfenv/versions/*
                  - /home/circleci/.tfenv/bin/terraform-*
      - when:
          # When it is hcl, the stack output path is the same as our .terraform-version
          condition: <<parameters.uses_raw_hcl>>
          steps:
            - save_cache:
                key: terraform-version-{{ checksum "<< parameters.stack-output-path >>/.terraform-version" }}
                paths:
                  - /home/circleci/.tfenv/versions/*
                  - /home/circleci/.tfenv/bin/terraform-*

  code_deploy_ecs:
    parameters:
      <<: [*repo_for_enum, *resource_class_enmum]
    # Our self hosted runners dont support docker images, cause its not deployed in kubernetes, so we have some special steps
    machine: true
    shell: /bin/bash --login -eo pipefail
    resource_class: << parameters.resource-class >>
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      # Restore TF_OUTPUT from workspace
      - attach_workspace:
          at: /tmp/workspace
      - run:
          # Pulls out the terraform params needed for CodeDeploy and then re-saves them to be used in the CodeDeploy ORB
          name: CodeDeploy Load Env
          command: |
            TF_OUTPUT=$(cat /tmp/workspace/tf_output.json)
            ECS_TASK_ARN=$(echo "$TF_OUTPUT" | jq -r '.["ecs-task-arn"].value')
            ECS_TASK_CONTAINER_NAME=$(echo "$TF_OUTPUT" | jq -r '.["ecs-task-containerName"].value')
            ECS_TASK_CONTAINER_PORT=$(echo "$TF_OUTPUT" | jq -r '.["ecs-task-containerPort"].value')
            ECS_TASK_FAMILY=$(echo "$TF_OUTPUT" | jq -r '.["ecs-task-family"].value')
            ECS_CODEDEPLOY_GROUP=$(echo "$TF_OUTPUT" | jq -r '.["ecs-codedeploy-group"].value')
            ECS_CODEDEPLOY_APP=$(echo "$TF_OUTPUT" | jq -r '.["ecs-codedeploy-app"].value')

            echo "export ECS_TASK_ARN=$ECS_TASK_ARN" >> $BASH_ENV
            echo "export ECS_TASK_CONTAINER_NAME=$ECS_TASK_CONTAINER_NAME" >> $BASH_ENV
            echo "export ECS_TASK_CONTAINER_PORT=$ECS_TASK_CONTAINER_PORT" >> $BASH_ENV
            echo "export ECS_TASK_FAMILY=$ECS_TASK_FAMILY" >> $BASH_ENV
            echo "export ECS_CODEDEPLOY_GROUP=$ECS_CODEDEPLOY_GROUP" >> $BASH_ENV
            echo "export ECS_CODEDEPLOY_APP=$ECS_CODEDEPLOY_APP" >> $BASH_ENV

      ## All the following steps are copied from https://github.com/CircleCI-Public/aws-ecs-orb/blob/master/src/commands/update_service.yml but we manually run them so we can pass env variables that we otherwise couldn't since it relies on parameter steps.
      ## See open issue to clean up when we can. https://github.com/CircleCI-Public/aws-ecs-orb/issues/211
      ## Once the above issue is fixed we can remove the included script and these commands and replace it with the below.
      # - aws-ecs/update_service:
      #     name: Deploy ECS Service
      #     codedeploy_application_name: ${ECS_CODEDEPLOY_APP}
      #     codedeploy_deployment_group_name: ${ECS_CODEDEPLOY_GROUP}
      #     codedeploy_load_balanced_container_name: ${ECS_TASK_CONTAINER_NAME}
      #     codedeploy_load_balanced_container_port: ${ECS_TASK_CONTAINER_PORT}
      #     family: ${ECS_TASK_FAMILY}
      #     container_image_name_updates: tag=${CIRCLE_SHA1}, container=${ECS_TASK_CONTAINER_NAME}
      #     deployment_controller: CODE_DEPLOY
      #     verify_revision_is_deployed: true

      - aws-cli/install
      ## Set the AWS CLI to use the Ecs Container credentials since this is in a hosted runner on AWS.
      - run:
          name: Setup AWS Auth
          command: |
            aws configure set profile.default.credential_source EcsContainer

      ## This will use the last revision in AWS to create a new task definition with the container image pointed to the sha of this commit
      ## If the task def was changed in terraform, that will be the latest revision that this will then over
      - aws-ecs/update_task_definition:
          family: ${ECS_TASK_FAMILY}
          container_image_name_updates: container=${ECS_TASK_CONTAINER_NAME},tag=${CIRCLE_SHA1}
      ## We need to checkout the code, because that contains our copied script from the aws-ecs orb repo.
      - checkout
      - run:
          name: Update ECS Blue/Green service with registered task definition.
          command: |
            .circleci/scripts/update_bluegreen_service_via_task_def.sh
          no_output_timeout: 10m
          environment:
            DEPLOYMENT_CONTROLLER: CODE_DEPLOY
            ORB_STR_CD_APP_NAME: ${ECS_CODEDEPLOY_APP}
            ORB_STR_CD_DEPLOY_GROUP_NAME: ${ECS_CODEDEPLOY_GROUP}
            ORB_STR_CD_LOAD_BALANCED_CONTAINER_NAME: ${ECS_TASK_CONTAINER_NAME}
            ORB_INT_CD_LOAD_BALANCED_CONTAINER_PORT: ${ECS_TASK_CONTAINER_PORT}
            # Dont wait for a successful deploy, we will get alerts in slack for those.
            ORB_BOOL_VERIFY_REV_DEPLOY: false
            ORB_STR_PROFILE_NAME: default
            ORB_BOOL_ENABLE_CIRCUIT_BREAKER: false
            ORB_STR_CD_CAPACITY_PROVIDER_NAME: 
            ORB_STR_CD_CAPACITY_PROVIDER_WEIGHT: 
            ORB_STR_CD_CAPACITY_PROVIDER_BASE: 
            ORB_STR_CD_DEPLOYMENT_CONFIG_NAME:

  code_deploy_lambda:
    parameters:
      codedeploy-app-name:
        description: CodeDeploy app name
        type: string
      codedeploy-group-name:
        description: CodeDeploy group name
        type: string
      function-name:
        description: >
          The name of the Lambda Function to deploy to
        type: string
      s3-bucket:
        type: string
        description: The name of the bucket to deploy from
      s3-key:
        type: string
        description: The name of the s3 key that contains the code to deploy
        default: ""
      function-alias:
        type: string
        description: The name of the lambda alias to use
        default: DEPLOYED
      <<: [*repo_for_enum, *resource_class_enmum]
    # Our self hosted runners dont support docker images, cause its not deployed in kubernetes, so we have some special steps
    machine: true
    shell: /bin/bash --login -eo pipefail
    resource_class: << parameters.resource-class >>
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - run:
          name: Deploy Lambda
          command: |
            export AWS_PAGER=""
            aws lambda wait function-updated --function-name '<< parameters.function-name >>'

            s3Key="<< parameters.s3-key >>"
            if [[ -z $s3Key ]]; then
                s3Key="$CIRCLE_SHA1.zip"
            fi

            aws lambda update-function-code \
                --function-name '<< parameters.function-name >>' \
                --s3-bucket '<< parameters.s3-bucket >>' \
                --s3-key "$s3Key"

            aws lambda wait function-updated --function-name '<< parameters.function-name >>'

            NEW_ENVVARS=$(aws lambda get-function-configuration --function-name '<< parameters.function-name >>' --query "Environment.Variables | merge(@, \`{\"GIT_SHA\":\"$CIRCLE_SHA1\"}\`)")
            aws lambda update-function-configuration --function-name '<< parameters.function-name >>' --environment "{ \"Variables\": $NEW_ENVVARS }"
            aws lambda wait function-updated --function-name '<< parameters.function-name >>'

            versionId=$(aws lambda publish-version \
                --function-name '<< parameters.function-name >>' | jq -r .Version)

            currentVersion=$(aws lambda get-alias \
                --function-name '<< parameters.function-name >>' \
                --name DEPLOYED | jq -r .FunctionVersion)

            app_spec_content_string="{'version':0.0,'Resources':[{'<< parameters.function-name >>':{'Type':'AWS::Lambda::Function','Properties':{'Name':'<< parameters.function-name >>','Alias':'<< parameters.function-alias >>','TargetVersion':'$versionId', 'CurrentVersion': '$currentVersion'}}}]}"
            echo "$app_spec_content_string"
            app_spec_content_sha256=$(echo -n "$app_spec_content_string" | shasum -a 256 | sed 's/ .*$//')
            revision="revisionType=AppSpecContent,appSpecContent={content=\"$app_spec_content_string\",sha256=$app_spec_content_sha256}"

            aws lambda wait function-updated --function-name '<< parameters.function-name >>'

            aws deploy create-deployment \
              --application-name="<< parameters.codedeploy-app-name >>" \
              --deployment-group-name="<< parameters.codedeploy-group-name >>" \
              --description="Triggered build $CIRCLE_SHA1 from CircleCI" \
              --revision="$revision"

  test_integrations:
    description: Run integration tests against external services, e.g. MySQL
    parameters:
      scope:
        description: The pnpm scope to run tests for
        type: string
      <<: *repo_for_enum
    docker:
      - image: *node_image
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
        environment:
          AWS_XRAY_LOG_LEVEL: silent
          AWS_XRAY_CONTEXT_MISSING: LOG_ERROR
      - image: redis:latest@sha256:3134997edb04277814aa51a4175a588d45eb4299272f8eff2307bbf8b39e4d43
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
      - image: mysql:8.0.36@sha256:92923e99278839ae86ba3edde972cf46f620ada05b37f85db04398cf8d7fc2b1
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
        environment:
          - MYSQL_ALLOW_EMPTY_PASSWORD=yes
          - TZ=UTC
        command: --default_authentication_plugin=mysql_native_password --sql-mode="NO_ENGINE_SUBSTITUTION" --character-set-server=UTF8MB3 --collation-server=utf8_unicode_ci
      - image: localstack/localstack:3.2.0@sha256:167eb023e07eef65f1e490d7a77cf45124e7a24395e4736dd2582e8ea0618ecb
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
        environment:
          SERVICES: s3,kinesis,sqs,dynamodb,sts,events,firehose,es
      - image: pocket/snowplow-micro:prod
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - checkout
      - install_pnpm
      - run:
          name: run setup.sh
          command: |
            export $(egrep -v '^#' .docker/local.env | xargs -0) && ./.circleci/scripts/setup.sh --db --aws
      - run:
          # Note there is a bug in turbo repo requiring a build https://github.com/vercel/turbo/issues/1609
          name: run tests
          command: |
            export $(egrep -v '^#' .docker/local.env | xargs -0)
            pnpm run test-integrations --filter=<< parameters.scope >>...

  build_image:
    description: Build and/or push docker image to ECR.

    parameters:
      aws-access-key-id:
        description: 'AWS access key id environment variable'
        type: string
      aws-region:
        description: 'AWS region value'
        type: string
      aws-secret-access-key:
        description: 'AWS secret access key environment variable'
        type: string
      ecr-url:
        description: 'The ecr url'
        type: string
      extra-build-args:
        description: 'Extra flags to pass to docker build. For examples, see https://docs.docker.com/engine/reference/commandline/build'
        type: string
        default: --build-arg GIT_SHA=${CIRCLE_SHA1}
      push:
        description: 'Whether or not to push the code'
        type: boolean
        default: false
      repo-name:
        description: 'The ecr repo name'
        type: string
      tag:
        description: 'The docker tag name'
        type: string
        default: latest,$CIRCLE_SHA1
      app_path:
        description: 'The path needed for building the Docker image'
        type: string
        default: '.'
      layer_caching:
        description: 'Whether to use docker layer caching'
        type: boolean
        default: true
      <<: *repo_for_enum
    executor: aws-cli/default

    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - checkout
      - aws-cli/setup:
          aws_access_key_id: << parameters.aws-access-key-id >>
          aws_secret_access_key: << parameters.aws-secret-access-key >>
          region: << parameters.aws-region >>
      - run:
          name: Setup common environment variables
          command: |
            { \
              echo 'export AWS_ECR_ACCOUNT_URL="<< parameters.ecr-url >>"'; \
              echo 'export REPO_NAME="<< parameters.repo-name >>"'; \
            } >> "$BASH_ENV"
      - when:
          condition: <<parameters.push>>
          steps:
            - aws-ecr/build-and-push-image:
                checkout: false
                repo: << parameters.repo-name >>
                path: << parameters.app_path >>
                setup-remote-docker: true
                remote-docker-layer-caching: << parameters.layer_caching >>
                aws-access-key-id: << parameters.aws-access-key-id >>
                aws-secret-access-key: << parameters.aws-secret-access-key >>
                tag: << parameters.tag >>
                remote-docker-version: default
                extra-build-args: << parameters.extra-build-args >>
      - unless:
          condition: <<parameters.push>>
          steps:
            - setup_remote_docker:
                version: default
                docker-layer-caching: << parameters.layer_caching >>
            - aws-ecr/build-image:
                repo: << parameters.repo-name >>
                tag: << parameters.tag >>
                path: << parameters.app_path >>
                extra-build-args: << parameters.extra-build-args >>

  build_lambda:
    description: Build and/or push lambda function.
    parameters:
      aws-access-key-id:
        description: 'AWS access key id environment variable'
        type: string
      aws-region:
        description: 'AWS region value'
        type: string
      aws-secret-access-key:
        description: 'AWS secret access key environment variable'
        type: string
      s3-bucket:
        description: 'The s3 bucket name'
        type: string
        default: ""
      scope:
        description: The pnpm scope to build for
        type: string
      sentry_project_name:
        type: string
        description: the Sentry project name
        default: ""
      sentry_env:
        type: string
        default: Prod
        description: Which environment the release is going to
      sentry_org:
        type: string
        description: The sentry org to upload source maps to
      s3-key:
        type: string
        description: The name of the s3 key that contains the code to deploy
        default: ""
      <<: *repo_for_enum
    docker:
      - image: *node_image
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - run:
          name: Setup Environment variables
          command: |
            echo "export SENTRY_AUTH_TOKEN="$SENTRY_BEARER"" >> "$BASH_ENV"
      - checkout
      - install_pnpm
      - run:
          # Theres a really annoying bug in PNPM deploy command that will try and create a folder at /home/pruned which we are not allowed to do, 
          # so we move it under 1 directory to let it do its thing.
          # https://github.com/pnpm/pnpm/issues/5086
          # We also go crazy on the deploy command per https://github.com/pnpm/pnpm/issues/6166#issuecomment-1802541463
          name: Build lambda
          command: |
            pnpm install --filter=<< parameters.scope >>... --frozen-lockfile
            pnpm run build --filter=<< parameters.scope >>...
            mkdir -p ~/bug/project
            cp -R . ~/bug/project/
            cd ~/bug/project/
            pnpm --config.shamefully-hoist=true --config.hoist=true --config.node-linker=true --config.symlinks=false --config.shared-workspace-lockfile=false deploy --filter=<< parameters.scope >> --prod pruned
      - when:
          condition: << parameters.sentry_project_name >>
          steps:
            - run:
                name: Inject Sentry & Upload Sourcemaps
                command: |
                  cd ~/bug/project/
                  pnpx @sentry/cli sourcemaps inject pruned/dist
                  pnpx @sentry/cli sourcemaps upload pruned/dist --release ${CIRCLE_SHA1} --auth-token ${SENTRY_AUTH_TOKEN} --org << parameters.sentry_org >> --project << parameters.sentry_project_name >>
      - run:
          name: Package Lambda
          command: |
            cd ~/bug/project/pruned
            cp -r package.json dist/
            cp -r node_modules/ dist/node_modules/

            cd dist
            zip -r9 ~/project/${CIRCLE_SHA1}.zip .
            mkdir /tmp/artifacts
            cp ~/project/${CIRCLE_SHA1}.zip /tmp/artifacts/
            cd ..
            maxFileSize=256000 # Get the size of the directory in kilobytes
            export dirSize=$(du -s dist | cut -f1) 
            echo "Size is: $dirSize"
            if ((dirSize > maxFileSize)); then
              echo "Directory size is equal to or larger than $maxFileSize KB. which is the lambda limit"
              exit 1
            fi 
      - when:
          condition: << parameters.s3-bucket >>
          steps:
            - aws-cli/setup:
                aws_access_key_id: << parameters.aws-access-key-id >>
                aws_secret_access_key: << parameters.aws-secret-access-key >>
                region: << parameters.aws-region >>
            - run:
                name: Upload Package
                command: |
                  s3Key="<< parameters.s3-key >>"
                  if [[ -z $s3Key ]]; then
                      s3Key="$CIRCLE_SHA1.zip"
                  fi
                  aws s3 cp $CIRCLE_SHA1.zip s3://<< parameters.s3-bucket >>/${s3Key}
      - store_artifacts:
          path: /tmp/artifacts

  apollo:
    description: >
      Runs Apollo rover schema check on the production graphql federated schema.
      If it is the production branch will deploy the subgraph to the production federated graph.
      If the branch is the development branch, will deploy the subgraph to the development federated graph.

    parameters:
      fed_graph_name:
        type: string
        description: The name of federated graph to check
      graph_name:
        type: string
        description: The name of this subgraph
      schema_file_path:
        type: string
        description: The patht to the schema file
        default: ./schema.graphql
      prod_graph_url:
        type: string
        description: The production subgraph url
      dev_graph_url:
        type: string
        description: The development subgraph url
      prod_graph_variant_name:
        type: string
        description: The production variant graph name
        default: "current"
      dev_graph_variant_name:
        type: string
        description: The development variant graph name
        default: "development"
      prod_branch:
        type: string
        description: The production git branch
        default: "main"
      dev_branch:
        type: string
        description: The development git branch
        default: "dev"
      apollo_key_env:
        type: env_var_name
        default: APOLLO_KEY
        description: The environment variable name of the apollo key to user
      build_command:
        description: 'build command to use if we need to'
        type: string
        default: ""

    docker:
      - image: *node_image
        auth:
          username: $DOCKER_LOGIN
          password: $DOCKER_PASSWORD

    steps:
      - checkout
      - run:
          name: install rover
          command: |
            # download and install Rover
            curl -sSL https://rover.apollo.dev/nix/latest | sh

            # This allows the PATH changes to persist to the next `run` step
            echo "export PATH=$HOME/.rover/bin:$PATH" >> "$BASH_ENV"
      - when:
          condition: << parameters.build_command >>
          steps:
            - install_pnpm
            - run:
                name: build schema
                command: |
                  << parameters.build_command >>
      - run:
          name: check service
          command: |
            export APOLLO_KEY=$<< parameters.apollo_key_env >>
            rover subgraph check << parameters.fed_graph_name >>@<< parameters.prod_graph_variant_name >> --schema << parameters.schema_file_path >> --name=<< parameters.graph_name >>
      - when:
          condition:
            equal: [<< parameters.prod_branch >>, << pipeline.git.branch >>]
          steps:
            - run:
                name: push service to prod
                command: |
                  export APOLLO_KEY=$<< parameters.apollo_key_env >>
                  rover subgraph publish << parameters.fed_graph_name >>@<< parameters.prod_graph_variant_name >> --schema << parameters.schema_file_path >> --routing-url << parameters.prod_graph_url >> --name=<< parameters.graph_name >>
      - when:
          condition:
            equal: [<< parameters.dev_branch >>, << pipeline.git.branch >>]
          steps:
            - run:
                name: push service to dev
                command: |
                  export APOLLO_KEY=$<< parameters.apollo_key_env >>
                  rover subgraph publish << parameters.fed_graph_name >>@<< parameters.dev_graph_variant_name >> --schema << parameters.schema_file_path >> --routing-url << parameters.dev_graph_url >> --name=<< parameters.graph_name >>

  sentry_release_notification:
    description: Create new release in Sentry
    resource_class: small
    parameters:
      sentry_project_name:
        type: string
        description: the Sentry project name
      sentry_env:
        type: string
        default: Prod
        description: Which environment the release is going to
      sentry_org:
        type: string
        description: The sentry org
      <<: *repo_for_enum
    docker:
      - image: getsentry/sentry-cli@sha256:6f67c389dd00c5fd44f99ba618a2f7d0490e8f40df2cc14c699afec3b7df61b2
        auth:
          username: $DOCKER_LOGIN
          password: $DOCKER_PASSWORD
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - run:
          name: Setup Environment variables
          command: |
            echo "export SENTRY_AUTH_TOKEN="$SENTRY_BEARER"" >> "$BASH_ENV"
            echo "export SENTRY_ORG=<< parameters.sentry_org >>" >> "$BASH_ENV"
            echo "export SENTRY_PROJECT=<< parameters.sentry_project_name >>" >> "$BASH_ENV"
      - run:
          name: Sentry Release Notification
          command: |
            source "$BASH_ENV"
            sentry-cli releases new "$CIRCLE_SHA1"
            sentry-cli releases set-commits "$CIRCLE_SHA1" --commit "Pocket/pocket-monorepo@$CIRCLE_SHA1"
            sentry-cli releases finalize "$CIRCLE_SHA1"
      - run:
          name: Sentry Deploy Notification
          command: |
            source "$BASH_ENV"
            sentry-cli releases deploys "$CIRCLE_SHA1" new -e "<< parameters.sentry_env >>"